# Deep Learningアルゴリズムまとめ

## RNN

### Simple RNN
古い情報については考慮されるようにはなっておらず、勾配消失問題が顕在。  

### LSTM
情報の取捨選択機構(ゲート)を持つRNNで、長期依存が必要なタスクについても適切な学習が可能。  
ゲート：入力ゲート、忘却ゲート、出力ゲート  
情報の取捨選択はシグモイド関数で行われる。  

### GRU(Gated Recurrent Unit)
LSTMをシンプルにしたモデルで、入力ゲートと忘却ゲートを１つの「更新ゲート」統合したもの。  

### Bi-directional RNN
過去の情報だけでなく未来の情報も学習することで精度を向上させるためのモデル。  
過去からの一方向での学習は、どのくらい前から予測すると誤差を最小化できるのかを決定するための「遅延パラメータ」を調節する必要があるが、そのようなパラメータに学習が依存することはないのでチューニングの手間が減る。

### Attention RNN

### Quasi-RNN
一般的なRNNでは一方向に過去の状態から遷移しないと、状態が計算できないため並列化しにくく学習に時間が掛かる。  
CNNの仕組みを生かして並列性を確保することで、高速かつ高精度なアーキテクチャ。  

## GAN